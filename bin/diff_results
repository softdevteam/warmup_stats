#!/usr/bin/python2.7

# Copyright (c) 2017 King's College London
# created by the Software Development Team <http://soft-dev.org/>
#
# The Universal Permissive License (UPL), Version 1.0
#
# Subject to the condition set forth below, permission is hereby granted to any
# person obtaining a copy of this software, associated documentation and/or
# data (collectively the "Software"), free of charge and under any and all
# copyright rights in the Software, and any and all patent rights owned or
# freely licensable by each licensor hereunder covering either (i) the
# unmodified Software as contributed to or provided by such licensor, or (ii)
# the Larger Works (as defined below), to deal in both
#
# (a) the Software, and
# (b) any piece of software and/or hardware listed in the lrgrwrks.txt file if
# one is included with the Software (each a "Larger Work" to which the Software
# is contributed by such licensors),
#
# without restriction, including without limitation the rights to copy, create
# derivative works of, display, perform, and distribute the Software and make,
# use, sell, offer for sale, import, export, have made, and have sold the
# Software and the Larger Work(s), and to sublicense the foregoing rights on
# either these or other terms.
#
# This license is subject to the following condition: The above copyright
# notice and either this complete permission notice or at a minimum a reference
# to the UPL must be included in all copies or substantial portions of the
# Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""
Plot a chart describing the effect of re-running an experiment with fewer iterations.
MUST be run after generate_truncated_json.
"""

import math
import os
import sys

# R packages are stored relative to the top-level of the repo.
if ('R_LIBS_USER' not in os.environ or 'rlibs' not in os.environ['R_LIBS_USER']):
    os.environ['R_LIBS_USER'] = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                                             'work', 'rlibs')
    args = [sys.executable]
    args.extend(sys.argv)
    os.execv(sys.executable, args)

# We use a custom install of rpy2, relative to the top-level of the repo.
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))),
                                'work', 'pylibs'))

import argparse
import json
import numpy

import rpy2
import rpy2.interactive.packages
import rpy2.robjects

sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from warmup.krun_results import parse_krun_file_with_changepoints
from warmup.latex import end_document, end_longtable, end_table, escape
from warmup.latex import get_latex_symbol_map, preamble
from warmup.latex import start_longtable, start_table, STYLE_SYMBOLS
from warmup.summary_statistics import BLANK_CELL, collect_summary_statistics
from warmup.summary_statistics import convert_to_latex, write_html_table

DESCRIPTION = lambda fname: """
Diff two Krun results files. Input files to this script should already have
outliers and changepoints marked (i.e. the mark_outliers_in_json and
mark_changepoints_in_json scripts should already have been run). Output
can be in HTML or LaTeX. A JSON file containing a raw diff is dumped to disk.

Example usage (input Krun results files, output HTML):

    $ python %s --json diff_summary.json --input-results before.json.bz2 after.json.bz2 --html diff.html

Example usage (input JSON summary file, output LaTeX):

    $ python %s --input-summary diff_summary.json --tex diff.tex
"""

ALPHA = 0.01  # Significance level.
CATEGORIES = ['warmup', 'slowdown', 'flat', 'no steady state']
MCI = rpy2.interactive.packages.importr('MultinomialCI')
# List indices (used in favour of dictionary keys).
CLASSIFICATIONS = 0  # Indices for top-level summary lists.
STEADY_ITER = 1
STEADY_ITER_VAR = 2
STEADY_STATE_TIME = 3
STEADY_STATE_TIME_VAR = 4
INTERSECTION = 5
SAME = 0  # Indices for nested lists.
DIFFERENT = 1
BETTER = 2
WORSE = 3
# Dictionary keys
BEFORE = 'before'
AFTER = 'after'
CLASSIFIER = 'classifier'
DIFF = 'diff'
# LaTeX output.
TITLE = 'Summary of benchmark classifications'
TABLE_FORMAT = ('ll@{\hspace{0cm}}ll@{\hspace{-1cm}}r@{\hspace{0cm}}r@{\hspace{0cm}}r@{\hspace{0cm}}r@{\hspace{0cm}}'
                'l@{\hspace{.3cm}}ll@{\hspace{-1cm}}r@{\hspace{0cm}}r@{\hspace{0cm}}rr@{\hspace{0cm}}r')
TABLE_HEADINGS_START1 = '\\multicolumn{1}{c}{\\multirow{2}{*}{}}&'
TABLE_HEADINGS_START2 = '&'
TABLE_HEADINGS1 = ('&&\\multicolumn{1}{c}{} &\\multicolumn{1}{c}{Steady} &\\multicolumn{1}{c}{Steady iter.} '
                   '&\\multicolumn{1}{c}{Steady} &\\multicolumn{1}{c}{Steady} &\\multicolumn{1}{c}{Steady perf.}')
TABLE_HEADINGS2 = ('&&\\multicolumn{1}{c}{Class.} &\\multicolumn{1}{c}{iter (\#)} &\\multicolumn{1}{c}{variation} '
                   '&\\multicolumn{1}{c}{iter (s)} &\\multicolumn{1}{c}{perf (s)} &\\multicolumn{1}{c}{variation}')

JSON_VERSION_NUMBER = '2'


def fatal(message):
    print(message)
    sys.exit(1)


def legend():
    key = (colour_tex_cell(BETTER, 'improved', legend=True),
           colour_tex_cell(WORSE, 'worsened', legend=True),
           colour_tex_cell(DIFFERENT, 'different', legend=True),
           colour_tex_cell(SAME, 'unchanged', legend=True))
    return '\\textbf{Diff against previous results:} ' + ' '.join(key) + '.'


def do_intervals_differ((x1, y1), (x2, y2)):
    """Given two IQRs or CIs return True if they do NOT overlap."""

    assert y1 >= x1 and y2 >= x2
    return y1 < x2 or y2 < x1


def does_interval_narrow((x1, y1), (x2, y2)):
    """Return True if the second interval is narrower than the first."""

    assert y1 >= x1 and y2 >= x2
    # Sometimes there is no variation in a dataset (more likely for steady iter
    # than steady perf, for obvious reasons). So, we check for special cases,
    # rather than causing a divide by zero.
    diff1, diff2 = y1 - x1, y2 - x2
    if diff1 == 0 and diff2 == 0:
        return SAME
    if diff1 == 0 and diff2 > 0:
        return WORSE
    ratio = float(diff2) / float(diff1)
    if ratio == 1.0:
        return SAME
    elif ratio < 1.0:
        return BETTER
    return WORSE


def do_mean_cis_differ(mean1, ci1, mean2, ci2):
    """Given two means +/- CIs return True if they do NOT overlap."""

    assert ci1 >= 0.0 and ci2 >= 0.0, 'Found negative confidence interval from bootstrapping.'
    x1 = mean1 - ci1
    y1 = mean1 + ci1
    x2 = mean2 - ci2
    y2 = mean2 + ci2
    return do_intervals_differ((x1, y1), (x2, y2))


def does_ci_narrow(mean1, ci1, mean2, ci2):
    """Return True if the second interval is narrower than the first."""

    assert ci1 >= 0.0 and ci2 >= 0.0, 'Found negative confidence interval from bootstrapping.'
    x1 = mean1 - ci1
    y1 = mean1 + ci1
    x2 = mean2 - ci2
    y2 = mean2 + ci2
    return does_interval_narrow((x1, y1), (x2, y2))


def all_flat(classifications):
    """Return True if all pexecs in a detailed classification dict are 'flat'."""

    return (classifications['warmup'] == 0 and classifications['slowdown'] == 0
            and classifications['no steady state'] == 0)


def all_nss(classifications):
    """Return True if all pexecs in a detailed classification dict are 'no steady state'."""

    return (classifications['warmup'] == 0 and classifications['slowdown'] == 0 and
            classifications['flat'] == 0)


def any_nss(classifications):
    """Return True if any pexec in a detailed classification dict is 'no steady state'."""

    return classifications['no steady state'] > 0


def diff(before_file, after_file, summary_filename):
    """Diff results in before_file and after_file."""

    classifiers = dict()
    before_results = None
    # In the JSON dump, we need the diff, and  the original summaries of the
    # before / after results, so that they can be written into a LaTeX table.
    summary = {DIFF: dict(), BEFORE: None, AFTER: None, CLASSIFIER: None}
    print('Loading %s.' % before_file)
    classifiers[BEFORE], before_results = parse_krun_file_with_changepoints([before_file])
    summary[BEFORE] = collect_summary_statistics(before_results,
                                                 classifiers[BEFORE]['delta'], classifiers[BEFORE]['steady'])
    print('Loading %s.' % after_file)
    classifiers[AFTER], after_results = parse_krun_file_with_changepoints([after_file])
    summary[AFTER] = collect_summary_statistics(after_results,
                                                classifiers[AFTER]['delta'], classifiers[AFTER]['steady'])
    for key in classifiers[BEFORE]:
        assert classifiers[BEFORE][key] == classifiers[AFTER][key], \
            'Results files generated with different values for %s' % key
    summary[CLASSIFIER] = classifiers[AFTER]
    assert len(before_results.keys()) == 1, 'Expected one machine per results file.'
    assert len(after_results.keys()) == 1, 'Expected one machine per results file.'
    assert before_results.keys()[0] == after_results.keys()[0], 'Expected results to be from same machine.'
    machine = before_results.keys()[0]
    # Generate CIs for DEFAULT_ITER classification data.
    before_class_cis = dict()
    for key in before_results[machine]['classifications']:
        if len(before_results[machine]['classifications'][key]) == 0:  # Skipped benchmark.
            continue
        class_counts = [before_results[machine]['classifications'][key].count(category) for category in CATEGORIES]
        before_class_cis[key] = numpy.array(MCI.multinomialCI(rpy2.robjects.FloatVector(class_counts), ALPHA))
    for key in after_results[machine]['classifications']:
        if len(after_results[machine]['classifications'][key]) == 0:  # Skipped benchmark.
            continue
        if key in before_results[machine]['classifications']:
            bench, vm = key.split(':')[:-1]
            if vm not in summary[DIFF]:
                summary[DIFF][vm] = dict()
            summary[DIFF][vm][bench] = [None, None, None, None, None, None]
    for key in after_results[machine]['classifications']:
        if len(after_results[machine]['classifications'][key]) == 0:  # Skipped benchmark.
            continue
        if not key in before_results[machine]['classifications']:
            continue
        bench, vm = key.split(':')[:-1]
        # Classifications are available, whether or not summary statistics can be generated.
        trunc_cat = [summary[AFTER]['machines'][machine][vm][bench]['process_executons'][p]['classification'] \
                     for p in xrange(len(summary[AFTER]['machines'][machine][vm][bench]['process_executons']))]
        trunc_counts = [trunc_cat.count(category) for category in CATEGORIES]
        after_class_cis = numpy.array(MCI.multinomialCI(rpy2.robjects.FloatVector(trunc_counts), ALPHA))
        sample = summary[AFTER]['machines'][machine][vm][bench]
        base_case = summary[BEFORE]['machines'][machine][vm][bench]
        for category in CATEGORIES:
            cat_index = CATEGORIES.index(category)
            if do_intervals_differ(before_class_cis[key][cat_index], after_class_cis[cat_index]):
                if (sample['detailed_classification']['warmup'] + sample['detailed_classification']['flat'] >
                        base_case['detailed_classification']['warmup'] + base_case['detailed_classification']['flat']):
                    summary[DIFF][vm][bench][CLASSIFICATIONS] = BETTER
                    break
                elif (sample['detailed_classification']['no steady state'] + sample['detailed_classification']['slowdown'] >
                        base_case['detailed_classification']['no steady state'] + base_case['detailed_classification']['slowdown']):
                    summary[DIFF][vm][bench][CLASSIFICATIONS] = WORSE
                    break
                else:
                    summary[DIFF][vm][bench][CLASSIFICATIONS] = DIFFERENT
                    break
        else:
            summary[DIFF][vm][bench][CLASSIFICATIONS] = SAME
        # If the CIs did not overlap, but the ONLY difference is in the number
        # of warmups / flats, we say the results were the same (because we see
        # warmups / flats are the same case).
        if summary[DIFF][vm][bench][CLASSIFICATIONS] != SAME and \
                base_case['detailed_classification']['slowdown'] == sample['detailed_classification']['slowdown'] and \
                base_case['detailed_classification']['no steady state'] == sample['detailed_classification']['no steady state']:
            summary[DIFF][vm][bench][CLASSIFICATIONS] = SAME
        # If the CIs do overlap, but the classification has moved from bad
        # inconsistent to good inconsistent, then we say the result was better.
        if summary[DIFF][vm][bench][CLASSIFICATIONS] == SAME and \
                base_case['detailed_classification']['no steady state'] > 0 and \
                sample['detailed_classification']['no steady state'] == 0:
            summary[DIFF][vm][bench][CLASSIFICATIONS] = BETTER
        # That completes the category data. The remaining logic deals with the
        # numerical data (time to reach a steady state, steady state time per
        # iteration), and produces an overall classification for this benchmark.
        # Case 1) All flat.
        if (all_flat(sample['detailed_classification']) and all_flat(base_case['detailed_classification'])):
            summary[DIFF][vm][bench][STEADY_ITER] = SAME
            if base_case['steady_state_time_ci'] is None:
                summary[DIFF][vm][bench][STEADY_STATE_TIME] = DIFFERENT
            elif do_mean_cis_differ(base_case['steady_state_time'], base_case['steady_state_time_ci'],
                                    sample['steady_state_time'], sample['steady_state_time_ci']):
                if sample['steady_state_time'] < base_case['steady_state_time']:
                    summary[DIFF][vm][bench][STEADY_STATE_TIME] = BETTER
                else:
                    summary[DIFF][vm][bench][STEADY_STATE_TIME] = WORSE
            else:
                summary[DIFF][vm][bench][STEADY_STATE_TIME] = SAME
                var = does_ci_narrow(base_case['steady_state_time'], base_case['steady_state_time_ci'],
                                     sample['steady_state_time'], sample['steady_state_time_ci'])
                summary[DIFF][vm][bench][STEADY_STATE_TIME_VAR] = var
        # Case 2) One ALL FLAT, one not.
        elif (all_flat(sample['detailed_classification']) or all_flat(base_case['detailed_classification'])):
            if (any_nss(sample['detailed_classification']) or any_nss(base_case['detailed_classification'])):
                summary[DIFF][vm][bench][STEADY_ITER] = DIFFERENT
            elif (all_flat(base_case['detailed_classification']) and
                  do_intervals_differ((1.0, 1.0), sample['steady_state_iteration_iqr'])):
                if sample['steady_state_iteration'] < base_case['steady_state_iteration']:
                    summary[DIFF][vm][bench][STEADY_ITER] = BETTER
                else:
                    summary[DIFF][vm][bench][STEADY_ITER] = WORSE
            elif (all_flat(sample['detailed_classification']) and
                  do_intervals_differ((1.0, 1.0), base_case['steady_state_iteration_iqr'])):
                if sample['steady_state_iteration'] < base_case['steady_state_iteration']:
                    summary[DIFF][vm][bench][STEADY_ITER] = BETTER
                else:
                    summary[DIFF][vm][bench][STEADY_ITER] = WORSE
            else:
                summary[DIFF][vm][bench][STEADY_ITER] = SAME
            if (any_nss(sample['detailed_classification']) or any_nss(base_case['detailed_classification'])):
                summary[DIFF][vm][bench][STEADY_STATE_TIME] = DIFFERENT
            elif do_mean_cis_differ(base_case['steady_state_time'], base_case['steady_state_time_ci'],
                                    sample['steady_state_time'], sample['steady_state_time_ci']):
                if sample['steady_state_time'] < base_case['steady_state_time']:
                    summary[DIFF][vm][bench][STEADY_STATE_TIME] = BETTER
                else:
                    summary[DIFF][vm][bench][STEADY_STATE_TIME] = WORSE
            else:
                summary[DIFF][vm][bench][STEADY_STATE_TIME] = SAME
                var = does_ci_narrow(base_case['steady_state_time'], base_case['steady_state_time_ci'],
                                     sample['steady_state_time'], sample['steady_state_time_ci'])
                summary[DIFF][vm][bench][STEADY_STATE_TIME_VAR] = var
        # Case 3) One contains an NSS (therefore no steady iter / perf available).
        elif (any_nss(sample['detailed_classification']) or any_nss(base_case['detailed_classification'])):
            pass
        # Case 4) All three measures should be available in both the DEFAULT_ITER and last_iter cases.
        else:
            # If n_pexecs is small, and the steady_iters are all identical,
            # we sometimes get odd IQRs like [7.000000000000001, 7.0], so
            # deal with this as a special case to avoid triggering the assertion
            # in do_intervals_differ.
            if len(set(sample['steady_state_iteration_list'])) == 1:
                fake_iqr = (float(sample['steady_state_iteration_list'][0]), float(sample['steady_state_iteration_list'][0]))
                if do_intervals_differ(base_case['steady_state_iteration_iqr'], fake_iqr):
                    if sample['steady_state_iteration'] < base_case['steady_state_iteration']:
                        summary[DIFF][vm][bench][STEADY_ITER] = BETTER
                    else:
                        summary[DIFF][vm][bench][STEADY_ITER] = WORSE
                else:
                    summary[DIFF][vm][bench][STEADY_ITER] = SAME
                summary[DIFF][vm][bench][STEADY_ITER_VAR] = SAME
            elif do_intervals_differ(base_case['steady_state_iteration_iqr'],
                                     sample['steady_state_iteration_iqr']):
                if sample['steady_state_iteration'] < base_case['steady_state_iteration']:
                    summary[DIFF][vm][bench][STEADY_ITER] = BETTER
                else:
                    summary[DIFF][vm][bench][STEADY_ITER] = WORSE
                var = does_interval_narrow(base_case['steady_state_iteration_iqr'], sample['steady_state_iteration_iqr'])
                summary[DIFF][vm][bench][STEADY_ITER_VAR] = var
            else:
                summary[DIFF][vm][bench][STEADY_ITER] = SAME
                var = does_interval_narrow(base_case['steady_state_iteration_iqr'], sample['steady_state_iteration_iqr'])
                summary[DIFF][vm][bench][STEADY_ITER_VAR] = var
            if do_mean_cis_differ(base_case['steady_state_time'], base_case['steady_state_time_ci'],
                                  sample['steady_state_time'], sample['steady_state_time_ci']):
                if sample['steady_state_time'] < base_case['steady_state_time']:
                    summary[DIFF][vm][bench][STEADY_STATE_TIME] = BETTER
                else:
                    summary[DIFF][vm][bench][STEADY_STATE_TIME] = WORSE
            else:
                summary[DIFF][vm][bench][STEADY_STATE_TIME] = SAME
            var = does_ci_narrow(base_case['steady_state_time'], base_case['steady_state_time_ci'],
                                 sample['steady_state_time'], sample['steady_state_time_ci'])
            summary[DIFF][vm][bench][STEADY_STATE_TIME_VAR] = var
        # Was the benchmark better or worse overall?
        if not (BETTER in summary[DIFF][vm][bench] or WORSE in summary[DIFF][vm][bench] or
                DIFFERENT in summary[DIFF][vm][bench]):
            summary[DIFF][vm][bench][INTERSECTION] = SAME
        elif BETTER in summary[DIFF][vm][bench] and not WORSE in summary[DIFF][vm][bench]:
            summary[DIFF][vm][bench][INTERSECTION] = BETTER
        elif WORSE in summary[DIFF][vm][bench] and not BETTER in summary[DIFF][vm][bench]:
            summary[DIFF][vm][bench][INTERSECTION] = WORSE
        else:
            summary[DIFF][vm][bench][INTERSECTION] = DIFFERENT
    with open(summary_filename, 'w') as fd:
        json.dump(summary, fd, ensure_ascii=True, indent=4)
        print('Saved: %s' % summary_filename)
    return summary


def colour_tex_cell(result, text, legend=False):
    """Colour a table cell containing `text` according to `result`."""

    assert result in (None, SAME, DIFFERENT, BETTER, WORSE)
    if legend:
        cmd = 'legendcell'
    else:
        cmd = 'ccell'
    if not text or result is None or result == SAME:
        return text
    if result == BETTER:
        colour = 'lightgreen'
    elif result == WORSE:
        colour = 'lightred'
    else:
        colour = 'lightyellow'
    return '\\%s{%s}{%s}' % (cmd, colour, text)


def write_latex_table(machine, all_benchs, summary, diff, tex_file, num_splits,
                      with_preamble=False, longtable=False):
    """Write a tex table to disk"""

    num_benchmarks = len(all_benchs)
    all_vms = sorted(summary.keys())
    num_vms = len(summary)

    # decide how to lay out the splits
    num_vms_rounded = int(math.ceil(num_vms / float(num_splits)) * num_splits)
    vms_per_split = int(num_vms_rounded / float(num_splits))
    splits = [[] for x in xrange(num_splits)]
    vm_num = 0
    split_idx = 0
    for vm_idx in xrange(num_vms_rounded):
        if vm_idx < len(all_vms):
            vm = all_vms[vm_idx]
        else:
            vm = None
        splits[split_idx].append(vm)
        vm_num += 1
        if vm_num % vms_per_split == 0:
            split_idx += 1

    with open(tex_file, 'w') as fp:
        if with_preamble:
            fp.write(preamble(TITLE))
            legends = get_latex_symbol_map() + ' \\\\ ' + legend()
            fp.write('\\centering %s' % legends)
            fp.write('\n\n\n')
            if not longtable:
                fp.write('\\begin{landscape}\n')
                fp.write('\\begin{table*}[hptb]\n')
                fp.write('\\vspace{.8cm}\n')
                fp.write('\\begin{adjustbox}{totalheight=12.4cm}\n')
        # Emit table header.
        heads1 = TABLE_HEADINGS_START1 + '&'.join([TABLE_HEADINGS1] * num_splits)
        heads2 = TABLE_HEADINGS_START2 + '&'.join([TABLE_HEADINGS2] * num_splits)
        heads = '%s\\\\%s' % (heads1, heads2)
        if longtable:
            fp.write(start_longtable(TABLE_FORMAT, heads))
        else:
            fp.write(start_table(TABLE_FORMAT, heads))
        split_row_idx = 0
        for row_vms in zip(*splits):
            bench_idx = 0
            for bench in sorted(all_benchs):
                row = []
                for vm in row_vms:
                    if vm is None:
                        continue # no more results
                    try:
                        this_summary = summary[vm][bench]
                    except KeyError:
                        last_cpt = BLANK_CELL
                        time_steady = BLANK_CELL
                        last_mean = BLANK_CELL
                        classification = ''
                    else:
                        if vm in diff and bench in diff[vm]:
                            classification = colour_tex_cell(diff[vm][bench][CLASSIFICATIONS], this_summary['style'])
                            last_cpt = colour_tex_cell(diff[vm][bench][STEADY_ITER], this_summary['last_cpt'])
                            steady_iter_var = colour_tex_cell(diff[vm][bench][STEADY_ITER_VAR], this_summary['steady_iter_var'])
                            time_steady = colour_tex_cell(diff[vm][bench][STEADY_ITER], this_summary['time_to_steady_state'])
                            last_mean = colour_tex_cell(diff[vm][bench][STEADY_STATE_TIME], this_summary['last_mean'])
                            steady_time_var = colour_tex_cell(diff[vm][bench][STEADY_STATE_TIME_VAR], this_summary['steady_time_var'])
                        else:
                            classification = this_summary['style']
                            last_cpt = this_summary['last_cpt']
                            steady_iter_var = this_summary['steady_iter_var']
                            time_steady = this_summary['time_to_steady_state']
                            last_mean = this_summary['last_mean']
                            steady_time_var = this_summary['steady_time_var']
                        classification = '\\multicolumn{1}{l}{%s}' % classification
                        if classification == STYLE_SYMBOLS['flat']:
                            last_cpt = BLANK_CELL
                            time_steady = BLANK_CELL
                    if last_cpt == '':
                        last_cpt = BLANK_CELL
                    if time_steady == '':
                        time_steady = BLANK_CELL
                    if last_mean == '':
                        last_mean = BLANK_CELL

                    if bench_idx == 0:
                        if num_benchmarks == 10:
                            fudge = 4
                        elif num_benchmarks == 12:
                            fudge = 5
                        else:
                            fudge = 0
                        vm_cell = '\\multirow{%s}{*}{\\rotatebox[origin=c]{90}{%s}}' \
                            % (num_benchmarks + fudge, vm)
                    else:
                        vm_cell = ''
                    row_add = [BLANK_CELL, vm_cell, classification, last_cpt,
                               steady_iter_var, time_steady, last_mean, steady_time_var]
                    if not row:  # First bench in this row, needs the vm column.
                        if vm in diff and bench in diff[vm]:
                            bname = colour_tex_cell(diff[vm][bench][INTERSECTION], bench)
                        else:
                            bname = bench
                        row.insert(0, escape(bname))
                    row.extend(row_add)
                    vm_idx += 1
                fp.write('&'.join(row))
                # Only -ve space row if not next to a midrule
                if not longtable and bench_idx < num_vms - 1:
                    fp.write('\\\\[-3pt] \n')
                else:
                    fp.write('\\\\ \n')
                bench_idx += 1
            if split_row_idx < vms_per_split - 1:
                if longtable:
                    fp.write('\\hline\n')
                else:
                    fp.write('\\midrule\n')
            split_row_idx += 1
        if longtable:
            fp.write(end_longtable())
        else:
            fp.write(end_table())
        if with_preamble:
            if not longtable:
                fp.write('\\end{adjustbox}\n')
                fp.write('\\end{table*}\n')
                fp.write('\\end{landscape}\n')
            fp.write(end_document())


def create_cli_parser():
    """Create a parser to deal with command line switches."""

    description = DESCRIPTION(os.path.basename(__file__))
    parser = argparse.ArgumentParser(description=description,
                                     formatter_class=argparse.RawTextHelpFormatter)
    parser.add_argument('-j', '--json', action='store', default='diff_summary.json',
                        type=str, help='JSON file in which to write diff summary.')
    parser.add_argument('-n', '--num-splits', action='store', default=1,
                        type=int, help='Number of horizontal splits (LaTeX only).')
    parser.add_argument('--without-preamble', action='store_true',
                        dest='without_preamble', default=False,
                        help='Write out only a LaTeX table, for inclusion in a larger document.')
    outputs = parser.add_mutually_exclusive_group(required=True)
    outputs.add_argument('--tex', action='store', type=str,
                         help='LaTeX file in which to write diff summary.')
    outputs.add_argument('--html', action='store', type=str,
                         help='HTML file in which to write diff summary.')
    inputs = parser.add_mutually_exclusive_group(required=True)
    inputs.add_argument('-s', '--input-summary', action='store', default=None,
                        type=str, help=('Read summary data from JSON file rather than '
                                        'generating from a two original results files.'))
    inputs.add_argument('-r', '--input-results', nargs=2, action='append', default=[], type=str,
                        help='Exactly two Krun result files (with outliers and changepoints).')
    return parser


if __name__ == '__main__':
    parser = create_cli_parser()
    options = parser.parse_args()
    diff_summary = None
    if options.html and options.without_preamble:
        print('--without-preamble only makes sense with LaTeX output. Ignoring.')
    if options.input_summary is None:
        if '_outliers' not in options.input_results[0][0]:
            fatal('Please run mark_outliers_in_json on file %s before diffing.' %
                  options.input_results[0][0])
        if '_outliers' not in options.input_results[0][1]:
            fatal('Please run mark_outliers_in_json on file %s before diffing.' %
                  options.input_results[0][1])
        if '_changepoints' not in options.input_results[0][0]:
            fatal('Please run mark_changepoints_in_json on file %s before diffing.' %
                  options.input_results[0][0])
        if '_changepoints' not in options.input_results[0][1]:
            fatal('Please run mark_changepoints_in_json on file %s before diffing.' %
                  options.input_results[0][1])
        diff_summary = diff(options.input_results[0][0], options.input_results[0][1], options.json)
    else:
        with open(options.input_summary, 'r') as fd:
            diff_summary = json.load(fd)
        if diff_summary is None:
            fatal('Could not open %s.' % options.input_summary)
    classifier = diff_summary[CLASSIFIER]
    if options.html:
        print('Writing data to: %s' % options.html)
        write_html_table(diff_summary[AFTER], options.html, diff=diff_summary[DIFF], previous=diff_summary[BEFORE])
    if options.tex:
        machine, bmarks, latex_summary = convert_to_latex(diff_summary[AFTER], classifier['delta'],
                                                          classifier['steady'], diff=diff_summary[DIFF],
                                                          previous=diff_summary[BEFORE])
        print('Writing data to: %s' % options.tex)
        write_latex_table(machine, bmarks, latex_summary, diff_summary[DIFF], options.tex,
                          options.num_splits, with_preamble=(not options.without_preamble),
                          longtable=True)
